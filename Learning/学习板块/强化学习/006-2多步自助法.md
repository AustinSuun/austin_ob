# 回顾：动态规划（DP） 和时序差分（ TD） 的关系

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250506165500131.png)

区别在于 DP 可以看到所有状态，基于所有状态的价值来学习；T D 通常不能获得所有的状态，通过采样数据更新价值函数

# 回顾：动态规划（DP）和时序差分（TD）的区别

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250506165800561.png)

DP 基于期望来计算，因为它能看到所有状态的价值；TD 基于采样数据估计出来的价值函数来做策略提升

# 回顾：蒙特卡洛方法&时序差分

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250506170022948.png)

介于两者之间的是 n 步时序差分法，下文多步时许差分预测

# 多步时序差分

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250506170155450.png)

多步时序差分可以采样多步后续数据，整合到 $g_{t}^n$，n 代表采样步数
N 足够大时相当于蒙特卡洛

## N 步累积奖励
- 考虑下列 n 步累计奖励，$n=1,2,3,\dots,\infty$
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250506170550424.png)

- 定义 n 步累积奖励 $g_{t}^{n}=r_{t}+\gamma r_{t+1}+\gamma^2r_{t+2}+\dots+\gamma^{n-1}_{t+n-1}+\gamma^nV(s_{t+n})$

- N 步时序差分学习 $V(s_{t})\leftarrow V(s_{t})+\alpha(g_{t}^{(n)}-V(s_{t}))$
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250506171228509.png)

## 随机游走的例子里面使用 n 步时序差分
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250506171319797.png)

**多步时序差分发的表现**

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250506171435438.png)

可以看到当 $n=4$ 时表现最好
问题：到底选择几步比较好？  下面有介绍综合多个长度采样数据的方法

## 平均 n 步累积奖励
- 我们可以对不同的 n 下的 n 步累计奖励求平均值
- 例如，求 2 步和 3 步的平均累计奖励
$$
\frac{1}{2}g^{(2)}+\frac{1}{2}g^{(3)}
$$
- 上式结合了两种不同的时间步长的信息
- 我们能否结合所有不同时间步长的信息？可以

## 使用平均 n 步累计奖励的 $TD(\lambda)$ 算法
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250506172233893.png)

先给对应的步长奖励乘上上 $\lambda^{t-1}$ ，再统一乘上 $(1-\lambda)$ 进行归一化，因为系数和是 $\frac{1}{1-\lambda}$
当 $\lambda$ 越大，越看短期的短距离的 TD ；$\lambda$ 越小，越看长期的长距离的 TD

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250506173010695.png)

上图是 $(1-\lambda)\lambda^{t-1}$ 系数随着时间步的变化

当 $\lambda=1$ 时，$g_{t}^{\lambda}=g_{t}$, 相当于蒙特卡洛
当 $\lambda=0$ 时，$g_{t}^{\lambda}=g_{t}^{(1)}$，相当于单步时序差分方法

## $TD(\lambda)vs.n$ 步时序差分
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250506173631017.png)

左边时序差分在 $\lambda=0.8$ 时表现更好，且比多步时许差分损失更低

- 使用最佳的 $\alpha$ 和 $\lambda$ 值时，离线 $\lambda-$ 累计奖励算法具有稍好一点的实验结果

# 多步 SARSA
- N 步的思想是否只能用在预测上？能不能把这种思想放在控制上？这样我们得到了n步 SASAS 算法
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250506174438810.png)

- 类似 n 步 TD，我们有：
$$
\begin{align}
g_{t:t+n}=r_{t}+\gamma r_{t+1}+\dots+\gamma^{n-1}r_{t+n-1}+\gamma^nQ(s_{t+n},a_{t+n})& \\ \\

\text{其中}n\geq1,0\leq t<T-n&
\end{align}
$$
$$
\begin{align}
Q(s_{t},a_{t})\leftarrow Q(s_{t},a_{t})+\alpha[g_{t: t+n}-Q(s_{t},a_{t})]& \\ \\

\text{其中}0\leq t<T-n&
\end{align}
$$
## 多步 SARSA 的例子
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250506175747303.png)

当只有一步 TD 的时候，如果进行了一步四周的收益都一样没有收益，那智能体看的很近，Q 函数值也很少更新，如果使用多步 TD 他就可以跨过一些步数看的更远，看到更远的收益，那么反向传播回去，整个路线上的 Q 函数都会更新值，学习的反而更快

# 使用重要性采样的多步离线学习法
多步的异策略学习
异策略学习法的一个中心问题是当前策略和采样策略之间数据分布不一致，因此可能需要进行重要性采样
## 回顾：采用重要性采样的离线学习
- 离线学习下通常需要两个策略
	- $\pi$：通常是对于当前的动作值估计函数的一个贪婪策略（也可能是随机策略）
	- $b$：用来收集数据，更有探索性的一个策略（常用 $\varepsilon-gre^{ dy }$ 实现）

- 由于收集数据的策略和我们要实际优化的策略不是一个策略，我们需要加上一项修正（值函数为例）
$$
\Delta V^\prime = \frac{\pi(a_{t}|s_{t})}{b(a_{t}|s_{t})}\Delta V
$$
- 在重要性采样的角度对值函数的推倒
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250506180936427.png)
---
$$
\rho_{t: h}=\prod_{k=t}^{min(h,T-1)}\frac{\pi(a_{k}|s_{k})}{b(a_{k}|s_{k})}
$$
- 对于状态值函数 V：
$$
V(s_{t})\leftarrow V(s_{t})+\alpha\rho_{t: t+n-1}[g_{t}^{(n)}-V(s_{t})]
$$
- 对于动作值函数 Q:
$$
Q(s_{t},a_{t})\leftarrow Q(s_t,a_{t})+\alpha\rho_{t: t+n-1}[g_{t}^{(n)}-Q(s_{t},a_{t})]
$$

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250506181813007.png)

当使用多步 S A R S A 的步数比较大的时候，由于重要性采样连乘，产生的方差是比较大的，是比较危险的方法

但是好的地方在于数据是由 b 策略采样出来的，b 策略通常是在几轮之前的 $\pi$ 策略，这样 $b$ 策略采样的数据分布和当前数据是比较接近的，可以通过选 b 策略采样数据的训练轮次，平衡重要性采样产生的偏差

多步SARSA 采样数据的效率变高了，数据的效率变高了（可以使用多轮），节约了数据；问题是带来了方差

不产生方差|不适用重要性采样的方法，即下面的多步树回朔算法
# 多步备份|回朔树算法（N-step Tree Backup Algorithm）

- 是否存在一种不需要使用重要性采样的离线算法？
	- Q 学习和期望S A R SA
- 我们可以把这种形式拓展到 n 步，称为多步树回朔算法
- 以 $n=3$ 为例
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250506183116031.png)

---
日期: 2025-05-12
作者:
  - Austin
tags:
---
# 回顾
![image.png|800](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250512075907358.png)

> 以上方法都是在更新价值函数，这些函数实际是保存到表格的数据|记录|词条|entry，对应不同状态下的价值;
> 
> 表格的方式容易直观理解, 表格中相邻的状态，在更新时没有，他们是完全**独立**的，这会导致更新时的低效性
> 例如，迷宫中两个相邻的位置，当其中一个更新时，不会影响另一个更新，但是他们效果相近，或者说有联系，我们期望，这种联系（他们相邻）可以让他自动更新，这样可以提高效率
> 又例如，同一状态下，两个可选择的动作a 效果非常接近，这时候只会更新更好的动作的价值

# 本节解决的关键问题

- 之前所有模型的做法都是基于创建一个查询表，在表中维护状态值函数 $V(s)$ 或状态-动作值函数 $Q(s,a)$

- 当处理大规模马尔可夫决策过程（MDP）时，即：
	- 状态后者状态-动作空间非常大
	- 连续的状态或者动作空间

- 是否仍然需要未每一个状态维护 $V(s)$ 或者为每个状态-动作对维护 $Q(s,a)$?
	例如
	- 围棋博弈（$10^{170}$ 的状态空间）
	- 直升机，自动驾驶汽车（连续的状态空间）

# 主要内容

**大规模马尔可夫巨测过程的解决方法**

- 对状态/动作进行离散化或分桶 (bucktization)
- 构建参数化的值函数估计

## 对状态/动作进行离散
### 离散化连续马尔可夫决策过程

对于连续状态马尔可夫决策过程，我们可以对状态空间进行离散化

- 例如，如果用 2 维连续值 $(s_{1},s_{2})$ 表示状态，可以使用网格对状态空间进行切分从而转化为离散的状态值
- 记离散的状态值为 $\bar{s}$
- 离散化的马尔可夫决策过程可以表示为：
$$(\bar{S},A,\{P_{sa}\},\gamma,R)\tag{五元组}$$
- 这样，就能使用前述的方法求解马尔可夫决策过程

![image.png|300](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250512195543857.png)

### 对大型马尔可夫决策过程分桶
对于一个大型的离散状态马尔可夫决策过程，我们可以对状态值进一步分桶以进行采样聚合

- 使用先验知识将相似的离散状态归类到一起
	- 例如，利用根据先验知识抽取出来的状态特征对状态进行聚类

![image.png|300](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250512200126986.png)

> 这里可以理解为用聚类根据状态特征聚合，对数据做一个预处理，相当于给各个特征划分了不同的状态空间区域，这个空间区域肯定是更小的；
> 但是当维度比较多的时候，聚类出的桶的数量也会数量爆炸

### 分桶/离散化

**优点**
- 操作简单直观
- 高效
- 在处理许多问题时能够达到较好的效果

**缺点**
- 过于简单的表示价值函数 $V$
- 可能为每个离散区间假设一个常数值
- 维度灾难
$$
S=R^n\Rightarrow\bar{S}=\{1,\dots,k\}^n
$$

![image.png|800](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250512201756022.png)

如图左，对状态空间进行离散化，目标价值是左下到右上的直线，但是由于离散化，路线变成了斜着的折线

既然目标函数是一个直线，那我们可以直接建模一个线性函数，可能只需要两个参数（k, b）就可以把他表示出来 ,采用分桶的方式需要多个直线（如图右） 来建模

## 参数化值函数估计
构建参数化（可学习的）函数来近似值函数
$$
\begin{align}
V_{\theta}(s)&\simeq V^\pi(s) \\
Q_{\theta}(s,a)&\simeq Q^\pi(s,a)
\end{align}
$$
- $\theta$ 是近似函数的参数，可以通过强化学习进行更新
- 参数化的方法将现有可见的状态泛化到没有见过的状态上

> 参数化的值函数优势在于，随着数据的增大，数据分布的改变，**参数的量是完全不变**，比如线性模型、神经网络，模型参数本身的维度 |参数量是不变的，只需要修改参数本身就可以了 ,在这种强化学习这种智能体不断学习过程当中, 数据分布一直再变化，数据量在扩大的场景下，非常适合，比较之下树模型、Boosting 的方法不适合，强化学习中很多时候使用的是参数化的函数方法
> 
> Boosting 方法随着与环境交互越来越多，这一类的模型会越来越大（因为数据是存到表格里的），此时他的数据相当于他的参数；或者环境交互的数据在不断地以分布的形式在变化的时候，需要舍弃更早的数据

### 值函数近似的主要形式
- 一些函数近似
	- 一般的线性模型
	- 神经网络
	- 决策树
	- 最近邻
	- 傅里叶/小波基底
- 可微函数
	- 一般的线性函数
	- 神经网络
- 我们希望模型射和在非稳态的，非独立同分布的数据上训练
	- 因此参数化模型比树模型更适合

![image.png|350](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250512204106467.png)

> 通常使用可微的函数来建模，这样可以通过梯度自动学习

### 基于随机梯度下降（SGD）的值函数近似
- 目标：找到参数向量 $\theta$ 最小化值函数近似值与真实值之间的均方误差
$$
J(\theta)=\mathbb{E}_{\pi}\left[\frac{1}{2}\left(V^{\pi}(s)-V_{\theta}(s)\right)^2\right]
$$
- 误差减小的梯度方向
$$
-\frac{\partial J(\theta)}{\partial \theta}=\mathbb{E}\left[\left(V^{\pi}(s)-V_{\theta}(s)\right)\frac{\partial{V_{\theta}(s)}}{\partial{\theta}}\right]
$$
- 单次采样进行随机梯度下降
$$
\begin{align}
\theta&\leftarrow \theta-\alpha\frac{\partial {J(s)}}{\partial \theta} \\
&=\theta+\alpha \left(V^\pi(s)-V_{\theta}(s)\right)\frac{\partial V_{\theta}(s)}{\partial \theta}
\end{align}
$$

> 这里我们需要关注的是 $V^\pi(s)$ 的取值，他是学习的目标，可以使用 MC 或者 TD 的方法来估计它的值
> $V_{\theta}$ 是模型的估计值，模型的输出值

### 特征化状态

用一个特征向量表示状态
$$
x(s)=\left[
\begin{array}{c}
x_{1}(s) \\
\vdots \\
x_{k}(s)
\end{array}
\right]
$$
以直升机控制问题为例
- 3 D 位置
- 3 D 速度（位置的变化量）
- 3 D 加速度（速度的变化量）

# 价值函数近似算法
## 状态值函数近似
### 线性状态值函数近似
- 用特征的线性组合表示价值函数
$$
V_{\theta}(s)=\theta^Tx(s)
$$
- 目标函数是参数 $\theta$ 的二次函数
$$
J(\theta)=\mathbb{E}\left[\frac{1}{2}(V^\pi(s)-\theta^Tx(s))^2\right]\tag{1}
$$
- 因而随机梯度下降能够收敛到全局最优解上
$$
\begin{align}
\theta&\leftarrow \theta-\alpha \frac{\partial J(\theta)}{\partial \theta} \\
&=\theta+\alpha(V^\pi(s)-V_{\theta}(s))x(s)
\end{align}
$$
其中，$\frac{\partial J(\theta)}{\partial \theta}=x(s)$，式 1 求导得
步长：$\alpha$
预测误差：$V^\pi(s)-V_{\theta}(s)$
特征值：$x(s)$ ，是状态空间的的特征向量

> 这里我们需要考虑目标值 $V^\pi(s)$ 需要填入什么值呢? 可以采用之前的方法 MC 或者TD如下

### 蒙特卡洛状态值函数近似
$$
\theta\leftarrow \theta+\alpha(\textcolor{red}{V^\pi(s)}-V_{\theta}(s))x(s)
$$
- 我们使用 $V^\pi(s)$ 表示真实的目标价值函数
- 在"训练数据"上运用监督学习对价值函数进行预测
$$
\langle s_{1},g_{1}\rangle,\langle s_{2},g_{2}\rangle, \dots,\langle s_{T},\textcolor \right{g_{T}}\rangle
$$
- 对于每一个数据样本 $\langle s_{t},G_{t}\rangle$
$$
\theta\leftarrow \theta+\alpha(\textcolor{red}{g_{t}}-V_{\theta}(s))x(s_{t})
$$
- 蒙特卡洛预测至少能收敛到一个局部最优解
	- 在价值函数为线性的情况下可以收敛到全局最优

> 蒙特卡洛方法可以使用 $g_{t}$ 近似表示目标价值

### 时序差分状态值函数近似


## 状态-动作值函数近似



## 案例分析
---
日期: 2025-05-07
作者:
  - Austin
tags:
---
#  回顾
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250507155218659.png)

## 策略值函数估计
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250507203652720.png)

## 策略提升
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250507203756695.png)

## 策略提升定理
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250507203955768.png)
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250507204118467.png)
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250507204233520.png)



# 规划与学习：入门算法和介绍
我们的策略改进一点之后，所有原本的价值函数都需要重新评估，这是非常贵的过程，哪有没有办法更好的评估它？这就为什么要做规划

我们会在环境模型下，做出规划的推演，强化模型中 model 都是环境模型
## 模型
- 给定一个状态和动作，模型能够预测下一个转台和奖励的分布：即 $p(s^\prime,r|s,a)$
	- $s,a$ ：给定的状态和动作
	- $r$：奖励
	- $s^\prime$：下一个状态
**模型的分类**
- 分布模型
	- 描述了轨迹的所有可能性及其概率
- 样本模型-黑盒（如神经网络构建）
	- 根据概率采样，只产生一条可能的轨迹
- 举例-掷骰子
	- 分布模型
		- 得到骰子数字总和的所有可能性及其概率
	- 样本模型
		- 只采样得到一种骰子数字综合

**模型的作用**
- 得到模拟的经验数据
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250507213052086.png)

智能体可以和环境模型交互。每次智能体基于当前状态选择一个动作，基于这个状态和动作，模型和真的环境一样可以采样下一个状态和收益，智能体不断地和环境模型交互

基于和模型的交互可以做规划（planning）

## 规划（Planning）
- 输入一个模型，输出一个策略的搜索过程
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250507213527582.png)
**规划的分类**
- 状态空间的规划（state-space planning）
	- 在状态空间搜索最佳策略，本课程重要围绕这中
- 规划空间的规划（plan-space planning）
	- 在规划空间搜索最佳策略，包括遗传算法和偏序规划
	- 这是，一个规划就是一个动作集合及动作顺序的约束
	- 这是的状态就是一个规划，目标状态就是能完成任务的规划

状态空间的规划是站在一个状态上转到另一个状态
规划空间的规划是站在一个序列|轨迹|动作序列上规划, 面对确定性的环境，如机器人

文本生成的时候两种方式相同，都是基于序列数据 

**规划的通用框架**
- 通过模型采样得到模拟数据
- 利用模拟数据更新值函数从而改进策略

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250507214447259.png)

**举例**
- 动态规划
	- 搜索整个状态空间，生成所有的状态转移分布
	- 状态转移分布回溯更新状态的值函数

**规划的好处**
- 任何时间可以被打断或者重定向
- 在复杂问题下，进行小且增量式的时间步规划是很有效的

## 规划与学习

- 不同点
	- 规划：利用模型产生的模拟经验
	- 学习：利用环境产生的真实经验
- 相同点
	- 通过回溯（back-up）更新值函数估计
	- 统一来看，学习的方法可以用在模拟经验上

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250507215647690.png)

Q 规划算法类比 Q 学习算法同样使用四元组 $(s,a,r,s^\prime,)$，只是 $s^\prime$ 是基于模型产生的

## Dyna（集成规划、决策和学习）
### **经验的不同用途**
- 更新模型
	- 模型学习，或间接强化学习
	- 对经验数据的需求少
- 更新值函数和策略
	- 直接强化学习（无模型强化学习）
	- 简单且不受模型偏差影响
![image.png|400](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250507220430200.png)

### **Dyna 的框架**
- 和环境交互产生真实经验
- 左边代表直接强化学习
	- 更新值函数和策略
- 右下角落边代表学习模型
	- 使用真实经验更新模型
- 右边代表基于模型的规划
	- 基于模型随机采样得到的模拟经验
		- 只从以前得到的==状态动作对==随机采样
	- 使用模拟经验做规划更新值函数和策略
![image.png|600](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250507221031168.png)

- $Model (s, a)$：预测 $(s,a)$ 对的下一个状态和奖励
- 步骤 (5)，(6) 去掉就是一时间步表格 Q 学习

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250507221435243.png)

基于值函数策略提升和规划可以一起做

Dyna-Q 是一步的 Q 学习和多步的 Q 规划 

（6）Q 规划是 n 个一步的模拟，模型模拟多步通常会出现符合误差问题（误差累积），模型只模拟一步

（5）是更新模型的参数，训练模型拟合环境

### **举例 1: 迷宫**
- 环境
	- 四个动作（上下左右）
	- 碰到障碍物和边界静止
	- 到达目标（G），得到奖励+1
	- 折扣因子 0.95

- 结果
	- 横轴代表游戏轮数
	- 纵轴代表到达 G 花的时间步长
	- 不同曲线代表不同的规划步长
	- 规划步长越长，表现收敛越快

![|600](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250508085954212.png)

### **为什么更快**
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250508090158561.png)

当智能体可以看的更远的时候，可以做 back up ，步长是 1 的时候一次只能更新一个状态（如左图），当步长更长的时候；更新可以影响到沿途的所有状态（如右图），使得学习更快

Q 学习是一步的，但是Q规划是多个一步，每次从之前采样的 s 出发

### **举例 2: 阻碍迷宫**
- 环境
	- 1000 步障碍向右移动
- 结果
	- 横轴代表时间步
	- 纵轴代表累计的收益
	- Dyna-Q+加了探索

![image.png|600](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250508091931051.png)

### **Dyna-Q+**
- 将奖励更改为 $r+\kappa\sqrt{\tau}$
	- $r$：原来得奖励
	- $\kappa$：小的权重参数
	- $\tau$：某个状态多久未到达过了

可以让智能体在规划的时候，更多的尝试没有走过的位置
当环境发生改变的时候，能够更快的适应



### **举例 3:捷径迷宫**
- 环境
	- 3000 步出现捷径
- 结果
	- Dyna-Q+能够发现捷径

![image.png|600](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250508092207916.png)



# 采样方法
## 常用的采样方法
- 均匀随机采样
- 模拟经验和更新集中在一些特殊的状态动作

随机采样的问题在于初始多数状态收益为零，如下图，初始可以更新的位置只有 useful 的点，其他位置需要等待收益从 useful 慢慢延伸过来，如果随机采样，学习效率很低

我们已经有了模型，可以让智能体对优先走的点做出判断

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250508092536777.png)

## 更好的采样方法
- 后向聚焦 (backward focusing)
	- 很多状态的值发生变化带动前几状态的值发生变化
- 有的值改变很多，有的改变很少
	- 因此需要根据紧急程度，给这些更新设置优先度进行更新

让模拟数据更加关注能让状态值发生改变的位置上
**优先级采样**
- 设置优先级更新队列
	- 根据值改变的幅度定义优先级：$P\leftarrow|r+\gamma\max_{a^\prime}Q(s^\prime,a^\prime)-Q(s,a)|$

## 采样方法：优先级采样

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250508093518058.png)

从环境采样出四元组，查看更新项的绝对值（价值函数的变化值），若超过阈值，将其插入到队列中，之后检查所有能够到达当前状态的状态，这些递归检查的状态使用模型生成的 $r,s^\prime$ 来更新价值

前提需要环境模型，如果基于之前的采样数据数据可能不支持这样做（无法得知前一个状态的 $s^\prime$）

**举例：迷宫**
- 横轴代表格子世界的大小
- 纵轴代表收敛到最优策略的更新次数
- 优先级采样收敛更快

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250508100110666.png)

**局限性及改进**
- 随机环境中利用期望更新（expected updates）的方法
	- 浪费很多计算资源在一些低概率的状态转移上
- 引入采样更新（sample updates）

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250508100132580.png)

Dyna-Q 的效率已经很高了，如果在 plan 的过程中使用优先级采样，效率会更高，需要的数据更小（如上图）

## 采样方法：期望更新和采样更新
- 值函数 $V(s)$
- 动作值函数：$Q(s,a)$
- 期望更新或者采样更新

**比较**
- 期望更新 $Q(s,a)\leftarrow \sum_{s^\prime,r}\hat{p}(s^\prime,r|s,a)[r+\gamma\max_{a^\prime}Q(s^\prime,a^\prime)]$
	- 需要分布模型
	- 需要更大的计算量
	- 没有偏差更准确
- 采样更新 $Q(s,a)\leftarrow Q(s,a)+\alpha[\gamma+\gamma\max_{a^\prime }Q(s^\prime,a^\prime)-Q(s,a)]$
	- 只需要采样模型
	- 计算量需求低
	- 受到采样误差（sampling error）的影响

期望更新需要直到状态转移概率，让环境模型学习的比较准，才能做好更新
采样更新不需要直到具体的概率分布，只需要用它采样出数据，进行类似 Q-planning 的方法，对 Q 函数进行更新，他的优势是它对环境模型的要求很低，他的更新速度很快，他没有动态规划那么准，但是实际中发现，环境模型不准是常态的，需要对环境模型本身有一个限制性的使用，比如 planning 的长度不要太长，当模型不太准的时候，近处的采样可以使用多次

![image.png|400](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250508101651293.png)

## 采样方法：轨迹采样
- 动态规划
	- 对整个状态空间进行遍历
	- 没有侧重实际需要关注的状态上
- 在状态空间中按照特定分布采样
	- 根据当前策略下所观测的分布进行才采样

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250508103128845.png)

**轨迹采样**
- 状态转移和奖励由模型决定
- 动作由当前的策略决定

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250508103200957.png)

- 优点
	- 不需要知道当前策略下状态的分布
	- 计算量少，简单有效
- 缺点
	- 不断重复更新已经被访问的状态

优点是简单，缺点是不知道概率分布，容易采样到概率高的样本，假设概率很低的一个样本奖励为-10000，实际是影响很大的样本，很可能采样不到，这样智能体对当前的估计是不准确的

使用采样还是哪怕使用重要性采样，也要让后继节点被采样到，要区 分它们的优劣

# 决策时规划
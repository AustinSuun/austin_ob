---
日期: 2025-05-22
作者:
  - Austin
tags:
---
> DDPG 和 TD 3 方法是针对连续动作空间的方法，使用链式求导更新网络参数。当动作是离散的或者策略本身是随机策略，这时候怎么办呢？

# 回顾
## 深度强化学习
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250522094320456.png)

- 直面理解：深度学习+强化学习
- 深度强化学习使强化学习算法能够以端到端的方式解决复杂问题
- 真正让强化学习有能力完成实际决策任务
- 比强化学习和深度学习都更加难以驯化
- 基于价值函数的深度强化学习
	-  DQN：一次输入多个行动 Q 值输出、目标网络、随机采样经验
	- Double DQN：解耦合行动选择和价值估计、解决 DQN 过高估计问题
	- Dueling DQN：精细捕捉价值和行动的细微关系、多种 advantage 函数建模

## 深度强化学习的分类
- 基于价值的方法
	- 深度 Q 网络及其扩展

- 基于确定性策略的方法
	- 确定性策略梯度（DPG），DDPG，TD 3

- **基于随机策略的方法**
	- **使用神经网络的策略梯度，自然策略梯度，信任区域策略优化（TRPO），近段策略优化（PPO），A 3 C**

# 基于神经网络的策略梯度
## 复习：策略梯度定理
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250522095154078.png)

## 策略网络的梯度
对于随机策略，一般采样到每一个行动的概率由 Softmax 实现
$$
\pi_{\theta}(a|s)=\frac{e^{f_{\theta}(s,a)}}{\sum_{a^\prime}e^{f_{\theta}(s,a^\prime)}}
$$
- 其中 $f_{\theta}(s,a)$ 是对状态-行动对的打分函数。由 $\theta$ 参数化，这可以通过一个**神经网络**实现

其 log 形式的梯度为
$$
\begin{align}
\frac{\partial \log \pi_{\theta}(a|s)}{\partial \theta}&=\frac{\partial f_{\theta}(s,a)}{\partial \theta}-\frac{1}{\sum_{a^\prime}e^{f_{\theta}(s,a^\prime)}}\sum_{a_{\prime\prime}}e^{f_{\theta}(s,a^{\prime\prime})}\frac{\partial f_{\theta}(s,a^{\prime\prime})}{\partial \theta} \\
&=\frac{\partial f_{\theta}(s,a)}{\partial \theta}-\mathbb{E}_{a^\prime\sim \pi_{\theta}(a^\prime|s)}\left[\frac{\partial f_{\theta}(s,a^\prime)}{\partial \theta}\right]
\end{align}
$$
> 可以看成当前动作的梯度减去当前状态下策略梯度的期望      

策略网络的梯度为
$$
\begin{align}
\frac{\partial J(\theta)}{\partial \theta}&=\mathbb{E}_{\pi_{\theta}}\left[\textcolor{red}{\frac{\partial \log \pi_{\theta}(s,a)}{\partial \theta}}Q^{\pi_{\theta}}(s,a)\right] \\
&=\mathbb{E}_{\pi_{\theta}}\left[\textcolor{red}{\left(\frac{\partial f_{\theta}(s,a)}{\partial \theta}-\mathbb{E}_{a^\prime\sim \pi_{\theta}(a^\prime|s)}\left[\frac{\partial f_{\theta}(s,a^\prime)}{\partial\theta}\right]\right)}Q^{\pi_{\theta}}(s,a)\right]
\end{align}
$$
## 策略梯度和 Q 学习的对比
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250522101858356.png)

> Q 学习|基于价值学习可以更快的拟合，基于策略梯度更难学习，但是最终效果会优于 Q 学习

## 复习 Actor-Critic
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250522102119557.png)

## 复习 A 2 C： Actor-Critic
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250522102209436.png)

> 使用 A 函数和 V 价值函数可以避免拟合 Q 函数，V 价值函数更加易于学习，以为输入只有状态 S，数值稳定性更好

# A 3 C：异步 A 2 C 方法

A 3 C 代表了异步优势动作评价（Asynchronous Advantage Actor Critic）
- 异步（Asynchronous）：因为算法设计并行执行一组环境
- 优势（Advantage）：策略更新使用优势函数
- 动作评价（Actor Critic）：使用 AC 方法，设计一个在学得的状态值函数帮助下进行更新的策略
$$
\nabla_{\theta^\prime}\log \pi(a_{t}|s_{t};\theta^\prime)A(s_{t},a_{t},\theta_{v})
$$
$$
A(s_{t},a_{t};\theta_{v})=\sum_{i=0}^{k-1}\gamma^ir_{t+1}+\gamma^kV(s_{t+k};\theta_{v})-V(s_{t};\theta_{v})
$$

![image.png|600](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250522103203830.png)

## A 3 C 对比实验
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250522103350666.png)

> A 3 C 是并行训练的方法，后来发展出了深度学习的并行框架 参数服务器

# 确定性梯度策略

当策略做策略梯度的时候，策略如果是神经网络，会存在什么问题

## 策略梯度算法回顾
![image.png|600](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250522103713192.png)

## 策略梯度的缺点

适合的步长在策略梯度中难以确定
- 采集到的数据的分布会随着策略的更新而变化
- 较差的步长产生的影响大

![image.png|600](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250522103752135.png)

> 问题在于，这种更新方式不能提前感知当前更新的方向是不是悬崖，按照策略梯度的固定步长可能会踏入到一个悬崖中，当进入悬崖之后，后续学习会在悬崖下采集数据，更差的数据导致学习变的更加低效，TROP 用来解决这个问题

# TROP
## 策略梯度的优化目标
优化目标的两种形式
- 第一种：$J(\theta)=\mathbb{E}_{\tau\sim p_{\theta}(\tau)}\left[ \sum_{t}\gamma^t r(s_{t},a_{t})\right]$
- 因为 $V^{\pi_\theta}(s)=\mathbb{E}_{a\sim \pi_{\theta}(s)}[Q^{\pi _\theta}(s,a)]=\mathbb{E}_{a\sim p_{\theta}(s)}\left[ \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)}\left[ \sum_{s_{k}=s,a_{k}=a}\sum_{t=k}^\infty \gamma^{t-k}r(s_{t},a_{t}) \right] \right]$
- 所以优化目标的第二种形式是：$J(\theta)=\mathbb{E}_{s_{0}\sim p_{\theta}(s_{0})}[V^{\pi_{\theta}}(s_{0})]$

> 第 2 种方式中的 Q 使用后续多条走 k 步的轨迹的期望来表示值函数

相关定义：
$\tau$ ：轨迹
$s_{0}$：初始状态
$s_{t},a_{t},r(s_{r},a_{t})$ ：$t$ 时刻的状态，动作和奖励
$\pi_{\theta}$：使用的策略
$\theta$：表示策略使用的参数
$Q^{\pi_{\theta}}$ 和 $V^{\pi_{\theta}}$：策略 $\pi_\theta$ 下的 Q 值与状态值函数

## 优化目标的优化量
$J(\theta^\prime)=\mathbb{E}_{\tau\sim p_\theta(\tau)}\left[ \sum_{t}\gamma^t r(s_{t},a_{t}) \right]$
$J(\theta)=\mathbb{E}_{s_{0}\sim p_{\theta}(s_{0})}[V^{\pi_{\theta}}(s_{0})]$

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250523150146318.png)

> $\theta^\prime$ 是新的网络的参数，是我们希望学习的方向的参数。以上是当前网络和网络最终的参数之间的误差|损失差
> 
> 因此 $\theta$ 一开始是未知的，所以我们也没有办法来采样它的数据轨迹，这时候为我们进行一步近似，使用 $\theta$ 采样，在使用重要性采样（这里近似的方法是强化学习的思想）

## 使用重要性采样
$$
\begin{align}
&J(\theta^\prime)-J(\theta) \\
&=\mathbb{E}_{\tau\sim p_{\theta^\prime}(\tau)}\left[ \sum_{t=0}^\infty \gamma^tA^{\pi_{\theta}}(s_{t},a_{t})\right] \\
&=\sum_{t}\mathbb{E}_{s_{t}\sim p_{\theta^\prime}(s_{t})}\left[ \mathbb{E}_{a_{t}\sim \pi_{\theta^\prime}(a_{t}|s_{t})}[\gamma^tA^{\pi_{\theta}}(s_{t},a_{t})] \right] \\
&=\sum_{t}\mathbb{E}_{s_{t}\sim p_{\theta^\prime}(s_{t})}\left[ \mathbb{E}_{a_{t}\sim \pi_{\theta}(a_{t}|s_{t})}[\frac{\pi_{\theta^\prime}(a_{t}|s_{t})}{\pi_{\theta}(a_{t}|s_{t})}\gamma^tA^{\pi_{\theta}}(s_{t},a_{t})] \right]
\end{align}
$$
![image.png|200](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250523153155018.png)


> 状态 S 依然使用 $\theta^\prime$ 来采样，动作使用重要性采样
> 上式把轨迹转换成当前状态和后续所有动作的期望

## 忽略状态分布的差异
> 我们默认使用当前的 $\theta$ 来采样，而不是最终的 $\theta^\prime$，这里进行了近似

当策略更新前后的变化较小时，可以令 $p_{\theta}(s_{t})\approx p_{\theta^\prime}(s_{t})$
- 假设使用确定性策略，当 $\pi_{\theta^\prime}(s_{t})\neq \pi_{\theta}({s_{t})}$ 的概率小于 $\varepsilon$ 时
- 或者假设使用随机策略，当 $a^\prime \sim \pi_{\theta^\prime}(\cdot|s_{t})\neq a\sim \pi_{\theta}(\cdot|s_{t})$ 的概率小于 $\varepsilon$ 时
- $p_{\theta^\prime}(s_{t})=(1-\epsilon)^tp_{\theta}(s_{t})+(1-(1-\epsilon)^t)|p_{mistake}(s_{t})-p_{\theta}(s_{t})|\leq_{2}(1-(1-\epsilon)^t)\leq 2\epsilon t$
> 最后一步近似，$(1-\epsilon)^t\geq 1-\epsilon t \ for \ \epsilon\in[0,1]$
> 这里证明了==当两个 $\theta$ 的差距不是很大的时候，他们的误差是可以被确定限制在一定的范围之内==


这样状态 s 的采样也使用 $\theta$ 采样来近似
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250523162823395.png)

> 这里的重要性采样，实际上不能真正的计算，以为 $\theta$ 未知，但是我们要知道进行这个操作。

## 约束策略的变化
使用 KL 散度约束策略更新的幅度

$$
\begin{align}
\theta^\prime\leftarrow \arg\max_{\theta^\prime}\mathbb{E}_{s_{t}\sim p_{\theta}(s_{t})}\left[ \mathbb{E}_{a_{t}\sim \pi_{\theta}(a_{t}|s_{t})}\left[ \frac{\pi_{\theta^\prime}(a_{t}|s_{t})}{\pi_{\theta}(a_{t}|s_{t})} \gamma^tA^{\pi_{\theta}}(s_{t},a_{t})\right] \right] \\
\end{align}
$$
$$
\text{such that }\mathbb{E}_{s_{t}\sim p_{\theta}(s_{t})}\left[ D_{KL}(\pi_{\theta^\prime}(a_{t}|s_{t}) ||\pi_{\theta}(a_{t}|s_{t})) \right]\leq\epsilon
$$
> $\theta$ 代表数据缓冲池里面的数据，$\theta^\prime$ 是当前学习的参数，$J(\theta^\prime)-J(\theta)$ 越大越好，说明学习到了更多
> 散度是衡量两个分布的相似程度：$D_{KL}(P||Q)=\sum_{x}P(x)\log\frac{P(x)}{Q(x)}$
> 两个分布差距越大，值越大
> 
> 这里限制两个策略，防止差距过大，差距过大需要更加谨慎
> 实际使用中，在 TRPO 是把这个限制条件作为惩罚项 ,这样直接梯度更新就可以


## TRPO 原理

通过计算当前位置的时候会考虑向海森阵一样的, 一个具体的周围的曲率. 考虑到曲率之后，曲率越大，说明特别容易踏空，周围的曲面非常扭曲，此时的梯度步长需要小一点|谨慎一点，防止踏空; 如果周围曲率很小，是平路, 步长大一点也没事

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250523170354063.png)

## 策略改进的单调性保证
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250523170818426.png)

> $D_{KL}^{max}$ 是当前状态下周围最大的散度
> 首先 $L_{\theta}(\theta^\prime)-C\cdot D_{KL}^{max}(\theta,\theta^\prime),where\ C=\frac{4\epsilon \gamma}{(1-\gamma)^2},max_{s,a}|A_{\pi}(s,a)|$ 是设定了一个下界，随着学习的进行，散度惩罚会越来越小，这个下界会逐渐接近 $J(\theta^\prime)$，同时梯度会向上移动，更接近最优位置


## 实验结果
### 训练曲线
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250523171945730.png)

> Trick：在一个节点 roll out 采样多个轨迹

### 结果比较

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250523172213804.png)

> TROP 解决的一个核心问题，策略一旦被更新，效果可能会适得其反（即不小心走进悬崖的情况），无法确定步长大小


# PPO-Proximal Policy Optimization
近端策略优化

## 回顾TRPO
TRPO 使用 KL 散度约束策略更新的幅度
$$
\begin{align}
&\theta^\prime\leftarrow\arg\max_{\theta^\prime}\sum_{t}\mathbb{E}_{s_{t}\sim \pi_{\theta}(a_{t}|s_{t})}\left[ \mathbb{E}_{a_{t}\sim \pi_{\theta}(a_{t}|s_{t})}\left[ \frac{\pi_{\theta^\prime}(a_{t}|s_{t})}{\pi_{\theta}(a_{t}|s_{t})} \gamma^tA^{\pi_{\theta}}(s_{t},a_{t})\right]\right] \\
&\text{such that }D_{KL}(\pi_{\theta^\prime}(a_{t}|s_{t})||\pi_{\theta}(a_{t}|s_{t}))\leq\epsilon
\end{align}
$$
- 使用 constraint violate as penaly（把限制条件|限制冲突作为惩罚项）
$$
\theta^\prime\leftarrow \arg\max_{\theta^\prime}\sum_{t}\mathbb{E}_{s_{t}\sim p_{\theta}(s_{t})}\left[ \mathbb{E}_{a_{t}\sim \pi_{\theta}(a_{t}|s_{t})}\left[ \frac{\pi_{\theta^\prime}(a_{t}|s_{t})}{\pi_{\theta}(a_{t}|s_{t})} \gamma^tA^{\pi_{\theta}}(s_{t},a_{t})\right] \right]-\lambda(D_{KL}(\pi_{\theta^\prime}(a_{t}|s_{t})||\pi_{\theta}(a_{t}|s_{t}))-\epsilon)
$$
	1. 优化上式，更新 $\theta^\prime$
	2. 更新 $\lambda\leftarrow \lambda+\alpha(D_{KL}(\pi_{\theta^\prime}(a_{t}|s_{t})||\pi_{\theta}(a_{t}||s_{t})-\epsilon)$

**TRPO 的不足**
- 重要性比例带来的大方差
- 求解约束优化问题的困难


> 更新 $\theta$ 的式子取了 max 操作，保证两个分布之间的最大的散度小于 $\epsilon$ 
> 
> 重要性采样的相乘的方式会带来更大的方差
> 求解约束需要单独计算非常麻烦，原版的计算算法中使用泰勒展开, 然后用共轭梯度的方法。后面对他进行了优化

## PPO 在 TRPO 基础上的改进

### 1 . 截断式优化目标
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250523175411307.png)

> 首先原本的 $\hat{A_{t}}$ 的估计就是不准确的，且重要性采样会带来方差，这两个数据本来就不准确，不如强行限制重要性采样的缩放数值，出让一定的偏差|bias 来减小方差|vanriance，这样做一个权衡
> 这样 $L^{CLIP}$ 可以被限制更大大的数值

### 2 . GAE-Generalized Advantage Estimation
优势函数 $\hat{A_{t}}$ 选用多步时序差分
$$
\textcolor{skyblue}{\hat{A_{t}}=-V(s_{t})+r_{t}+\gamma r_{t+1}+\cdots+\gamma^{T-t+1}r_{T-1}+\gamma^{T-t}V(s_{T}) }
$$
- 每次迭代中，并行 N 个 actor 收集 T 步经验数据
- 计算每步的 $\hat{A_{t}}$ 和 $L^{CLIP}(\theta)$，后称 mini_batch
- 更新参数 $\theta$，并更新 $\theta_{old}\leftarrow theta$

### 3. 自适应的 KL 惩罚项参数
$$
L^{KLPEN}(\theta)=\hat{\mathbb{E}}_{t}\left[ \frac{\pi_{\theta^\prime}(a_{t}|s_{t})}{\pi_{\theta}(a_{t}|s_{t})}\hat{A_{t}-\textcolor{skyblue}{\beta KL[\pi_{\theta_{old}}(\cdot|s_{t})|\pi_{\theta}(\cdot|s_{t})]}} \right]
$$
动态调整 $\textcolor{skyblue}{\beta}$ 方法
- 计算 KL 值 $d=\hat{\mathbb{E}}_{t}\left[ KL[\pi_{\theta_{old}}(\cdot|s_{t})|\pi_{\theta}(\cdot|s_{t})] \right]$
	A)  如果 $d<d_{targ}/1.5$，更新 $\beta\leftarrow \beta/{2}$
	B) 如果 $d>d_{targ}\times 1.5$，更新 $\beta\leftarrow \beta \times 2$

> 这里是计算的当前的散度，和之前的散度相比，如果散度已经是原来的 2/3，那么 $\beta$ 也要缩小（惩罚变小，别变得更加松散）；同样散度变大了，$\beta$ 也要变大（惩罚加大，变得更加谨慎）


## PPO 实验对比

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250523184250703.png)

- 在 MuJoCo 实验环境中的对比结果（训练 100 万步）

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250523185129017.png)

## PPO 在ChatGPT 中的使用

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250523185248906.png)

# 总结深度策略梯度方法
- 相比价值函数学习最小化 TD 误差的目标，策略梯度方法直接优化策略价值的目标更加贴合强化学习本质的目标
- 分布式的 actor-critic 算法能够充分利用多核 CPU 资源采样环境的经验数据，利用 GPU 资源异步的更新网络，这有效提升了 DRL 的训练效率
- 基于神经网络的策略在优化时容易以为异步走的太大而变得很差，进而下一轮产色行很低质量的经验数据，进一步无法学习好
- Trust Region 一类方法限制一步更新前后策略的差距（用 KL 散度），进而对策略价值做稳步提升
- PPO 在TRPO 的基础上进一步通过限制重要性采样的 range，构建优化目标的下界，进一步保证优化的稳定效果，是目前最常用的策略梯度算法
---
日期: 2025-05-22
作者:
  - Austin
tags:
---
> DDPG 和 TD 3 方法是针对连续动作空间的方法，使用链式求导更新网络参数。当动作是离散的或者策略本身是随机策略，这时候怎么办呢？

# 回顾
## 深度强化学习
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250522094320456.png)

- 直面理解：深度学习+强化学习
- 深度强化学习使强化学习算法能够以端到端的方式解决复杂问题
- 真正让强化学习有能力完成实际决策任务
- 比强化学习和深度学习都更加难以驯化
- 基于价值函数的深度强化学习
	-  DQN：一次输入多个行动 Q 值输出、目标网络、随机采样经验
	- Double DQN：解耦合行动选择和价值估计、解决 DQN 过高估计问题
	- Dueling DQN：精细捕捉价值和行动的细微关系、多种 advantage 函数建模

## 深度强化学习的分类
- 基于价值的方法
	- 深度 Q 网络及其扩展

- 基于确定性策略的方法
	- 确定性策略梯度（DPG），DDPG，TD 3

- **基于随机策略的方法**
	- **使用神经网络的策略梯度，自然策略梯度，信任区域策略优化（TRPO），近段策略优化（PPO），A 3 C**

# 基于神经网络的策略梯度
## 复习：策略梯度定理
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250522095154078.png)

## 策略网络的梯度
对于随机策略，一般采样到每一个行动的概率由 Softmax 实现
$$
\pi_{\theta}(a|s)=\frac{e^{f_{\theta}(s,a)}}{\sum_{a^\prime}e^{f_{\theta}(s,a^\prime)}}
$$
- 其中 $f_{\theta}(s,a)$ 是对状态-行动对的打分函数。由 $\theta$ 参数化，这可以通过一个**神经网络**实现

其 log 形式的梯度为
$$
\begin{align}
\frac{\partial \log \pi_{\theta}(a|s)}{\partial \theta}&=\frac{\partial f_{\theta}(s,a)}{\partial \theta}-\frac{1}{\sum_{a^\prime}e^{f_{\theta}(s,a^\prime)}}\sum_{a_{\prime\prime}}e^{f_{\theta}(s,a^{\prime\prime})}\frac{\partial f_{\theta}(s,a^{\prime\prime})}{\partial \theta} \\
&=\frac{\partial f_{\theta}(s,a)}{\partial \theta}-\mathbb{E}_{a^\prime\sim \pi_{\theta}(a^\prime|s)}\left[\frac{\partial f_{\theta}(s,a^\prime)}{\partial \theta}\right]
\end{align}
$$
> 可以看成当前动作的梯度减去当前状态下策略梯度的期望      

策略网络的梯度为
$$
\begin{align}
\frac{\partial J(\theta)}{\partial \theta}&=\mathbb{E}_{\pi_{\theta}}\left[\textcolor{red}{\frac{\partial \log \pi_{\theta}(s,a)}{\partial \theta}}Q^{\pi_{\theta}}(s,a)\right] \\
&=\mathbb{E}_{\pi_{\theta}}\left[\textcolor{red}{\left(\frac{\partial f_{\theta}(s,a)}{\partial \theta}-\mathbb{E}_{a^\prime\sim \pi_{\theta}(a^\prime|s)}\left[\frac{\partial f_{\theta}(s,a^\prime)}{\partial\theta}\right]\right)}Q^{\pi_{\theta}}(s,a)\right]
\end{align}
$$
## 策略梯度和 Q 学习的对比
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250522101858356.png)

> Q 学习|基于价值学习可以更快的拟合，基于策略梯度更难学习，但是最终效果会优于 Q 学习

## 复习 Actor-Critic
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250522102119557.png)

## 复习 A 2 C：又是 Actor-Critic
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250522102209436.png)

> 使用 A 函数和 V 价值函数可以避免拟合 Q 函数，V 价值函数更加易于学习，以为输入只有状态 S，数值稳定性更好

# A 3 C：异步 A 2 C 方法

A 3 C 代表了异步优势动作评价（Asynchronous Advantage Actor Critic）
- 异步（Asynchronous）：因为算法设计并行执行一组环境
- 优势（Advantage）：策略更新使用优势函数
- 动作评价（Actor Critic）：使用 AC 方法，设计一个在学得的状态值函数帮助下进行更新的策略
$$
\nabla_{\theta^\prime}\log \pi(a_{t}|s_{t};\theta^\prime)A(s_{t},a_{t},\theta_{v})
$$
$$
A(s_{t},a_{t};\theta_{v})=\sum_{i=0}^{k-1}\gamma^ir_{t+1}+\gamma^kV(s_{t+k};\theta_{v})-V(s_{t};\theta_{v})
$$

![image.png|600](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250522103203830.png)

## A 3 C 对比实验
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250522103350666.png)

> A 3 C 是并行训练的方法，后来发展出了深度学习的并行框架参数服务器

# 确定性梯度策略

当策略做策略梯度的时候，策略如果是神经网络，会存在什么问题

## 策略梯度算法回顾
![image.png|600](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250522103713192.png)

## 策略梯度的缺点

适合的步长在策略梯度中难以确定
- 采集到的数据的分布会随着策略的更新而变化
- 较差的步长产生的影响大

![image.png|600](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250522103752135.png)

> 问题在于，这种更新方式不能提前感知当前更新的方向是不是悬崖，按照策略梯度的固定步长可能会踏入到一个悬崖中，当进入悬崖之后，后续学习会在悬崖下采集数据，更差的数据导致学习变的更加低效，TROP 用来解决这个问题

# TROP
## 策略梯度的优化目标
优化目标的两种形式
- 第一种：$J(\theta)=\mathbb{E}_{\tau\sim p_{\theta}(\tau)}$
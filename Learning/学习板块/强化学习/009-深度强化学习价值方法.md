---
日期: 2025-05-15
作者:
  - Austin
tags:
  - 深度强化学习
---
# 课程回顾
## 基于表格的强化学习

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250515172034598.png)

## 价值和策略的近似逼近方法

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250515172242131.png)

### 价值和策略近似

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250515172555529.png)

- 直接使用深度神经网络建立这些近似函数，这回带来哪些挑战，以及怎么解决？

### 端到端强化学习

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250515172742383.png)


# 深度强化学习
- 深度强化学习
	- 利用生度神经网络进行价值函数和策略近似
	- 从何使强化学习以端到端的方式解决复杂问题
## 深度强化学习带来的关键变化

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250515174035504.png)

假如将深度学习（RL）和强化学习（RL）结合在一起会发生什么？
- 价值函数和策略现在变成了深度神经网络
- 相当高维的参数空间
- 难以稳定的训练
- 容易过拟合
- 需要大量的数据
- 需要高性能计算
- CPU （用于收集经验数据）和GPU（用于训练神经网络）之间的平衡
- ...

> 深度神经网络本身的黑盒性，还有动态环境的黑盒性，多智能体学习，博弈中的纳什平衡的不确定性，这些不确定性增大了学习的难度

## 深度强化学习的分类
- **基于价值的方法**
	- **深度 Q 网络及其扩展

- 基于确定性策略的方法
	- 确定性策略梯度（DPG），DDPG，TD 3

- 基于随机策略的方法
	- 使用神经网络的策略梯度，自然策略梯度，信任区域策略优化（TRPO），近段策略优化（PPO），A3C

# 深度 Q 网络
## 深度 Q 网络（DQN）
### Q 学习回顾
- 不直接更新策略
- 基于值的方法
- Q 学习算法学习一个由 $\theta$ 作为参数的函数 $Q_{\theta}(s,a)$
	- Target 值 $y_{t}=r_{t}+\gamma \max_{a^\prime}Q_{\theta}(s_{t+1},a^\prime)$
	- 更新方程 $Q_{\theta}(s_{t},a-t)\leftarrow Q_{\theta}(s_{t},a_{t})+\alpha(r_{t}+\gamma \max_{a^\prime}Q_{\theta}(s_{t+1},a^\prime)-Q_{\theta}(s_{t},a_{t}))$
	- 优化目标
$$
\theta^\star\leftarrow \arg\min_{\theta} \frac{1}{2}\sum_{(s_{t},a_{t})\in D}(Q_{\theta}(s_{t},a_{t})-(r+\gamma \max_{a^\prime}\textcolor{red}{Q_{\theta}(s_{t+1},a^\prime)}))^2
$$
 $\textcolor{red}{Q_{\theta}(s_{t+1},a^\prime)}$ _处不参与计算梯度_

> Q 学习是离线学习。允许一定程度的脱离当前策略的分布。这样的特性和深度学习更加契合。
> 因为按照之前的 SARSA 算法，Q 函数是按照表格的方式进行更新的，进行一次的学习之后，Q 函数|表格中只有一部分数据发生了变化，而深度 Q 网络，学习一次之后，所有的模型参数都会发生改变，那么所有的数据就需要重新采样。
> 对于 Q 学习来说，只需要维持一个 data repaly buffer（数据缓冲池，可以维持一个较大的空间，来存最近采样到的数据，更远时间的数据可以逐渐切除），
---

**直观想法**
使用神将网络来比进上述 $Q_{\theta}(s,a)$
- 算法不稳定性
	- 连续采样得到的 $\{(s_{t},a_{t},s_{t+1},r_{t})\}$ 不满足独立分布
	- $\{(s_{t},a_{t},s_{t+1},r_{t})\}$ 是状态-动作-下一状态-回报输入
	- $Q_{\theta}(s,a)$ 的频繁更新

**解决方法**
- 经验回收
- 使用双网络结构：评估网络（**evaluation network**）和目标网络（**target network**）

> 关键在于局部采样出来的数据与全局的分布不一致，神经很容易去拟合到刚刚采样到的数据上，因为智能体和环境交互的过程中，采样不是均匀地, 不是全局的分布。而强化学习需要边采样边进行学习，这能提很容易拟合到最近的数据上，而其他数据的情况会发散掉
> 解决方法有两个，经验回放需要从数据缓冲池中按照一定策略|战术|方法抽取数据，抽取的方法用来克服数据分布的偏差
> 双网络结构，一个作为当前训练的网络，一个作为评估网络，评估网络不用每轮都更新，间隔一些轮次更新成单前训练的网络

### 经验回放
经验回放
- 存储训练过程中的每一步 $e_{t}=(s_{t},a_{t},s_{t+1},r_{t})$ 于数据库 $D$ 中，采样时服从均匀分布

**优先经验回放**
衡量标准
- 以 Q 函数的值和 Target 值的差异来衡量学习的价值，即
$$
p_{t}=|r_{t}+\gamma\max_{a^\prime}Q_{\theta}(s_{t+1},a^\prime)-Q_{\theta}(s_{t},a_{t})|
$$
- 为了使各个样本都有机会被采样，存储 $e_{t}=(s_{t},a_{t},r_{t},p_{t}+\epsilon)$

选中的概率
- 样本 $e_{t}$ 选中的概率为 $P(t)=\frac{p_{t}^\alpha}{\sum_{k=1}^{N}p_{k}^\alpha}$

重要性采样
- 权重为 $\omega_{j}=\frac{(N\times P(j))^\beta}{ \max_{i}\omega_{i}}$

> 我们的目标是从数据缓冲池中采样出部分数据组成 mini batch 去更新 Q 函数。但是每次采样的时候肯定会碰到没有用的 data，那有没有办法每次都找到对我们学习很有用的数据，但是又等价于每次平均的从缓冲池中采样数据，答案是可以的，使用重要性采样的方法采样
> 
> 就是按照优先性选择样本，但是在学习的时候使用重要性采样给他回归到平均采样的权重
> 
> 这样的好处，对于一些采样概率很低，但是奖励很高的动作，我们能够采样到它，这样能够提升学习效率

算法：
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250515202345027.png)

### 目标网络
目标网络 $Q_{\theta}-(s,a)$
- 使用较旧的参数，记为 $\theta^-$，每隔 $C$ 步和训练网络同步一次
- 第 $i$ 次迭代的损失函数为
$$
L_{i}(\theta_{i})\mathbb{E}_{s_{t},s_{t+1},r_{t},p_{t}\sim D}\left[ \frac{1}{2}\omega _{t}(r_{t}+\gamma \max_{a^\prime}Q_{\theta_{i}^-}(s_{t+1},a^\prime)-Q_{\theta_{i}}(s_{t},a_{t}))^2\right]
$$

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250515204326477.png)

> 目标网络需要间隔几轮之后再同步，如果间隔较短，容易拟合到刚刚更新的数据上，导致 bias 很大


### 算法流程
1. 收集数据：使用 $\epsilon-greedy$ 策略进行探索，将得到的状态动作组 $(s_{t},a_{t},s_{t+1},r_{t})$ 放入经验池（replay-buffer）

2. 采样：从数据库中采样 $k$ 个动作状态组

3. 更新网络
	- 用采样得到的数据计算 $Loss$
	- 更新 Q 函数网络 $\theta$
	- 每 $C$ 次迭代（更新 Q 函数网络）更新一次目标网络 $\theta^-$

重复以上步骤

> 数据缓冲池可以使一个比较大的空间，来缓冲数据，更老的数据会逐渐切割，被新的数据覆盖 
> 
> 每 $C$ 次对目标网络做一个参数值的拷贝，在 $C$ 次之内更新评估网络的参数。这样评估网络学习的过程中每一个小阶段，目标网络都是稳定的|固定的，学习过程中目标应该是一个比较稳定的目标网络；如果马上更新了一部分参数，又把它作为目标来学习，这是很不靠谱的

在 Atari 环境中的实验结果
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250515212525895.png)


## Double DQN

### Q-learning 中的过高估计
- Q 函数的过高估计
	- Target 值 $$y_{t}=r_{t}+\gamma \max_{a^\prime}Q_{\theta}(s_{t+1},a^\prime)$$
- 过高估计的原因
$$
\max_{a^\prime\in A}Q_{\theta}(s_{t+1},a^\prime)=Q_{\theta^\prime}(s_{t+1},\arg\max_{a^\prime}Q(s_{t+1},a^\prime))
$$
	> 在 $\arg \max$ 选择的 $a^\prime$ 可能会由于 Q 函数错误的过高估计导致，如刚开始训练的时候，错误的估计值且数值很大，导致一直选择这个错误的动作，导致过高估计

- Q 函数的过高估计程度随着候选行动数量增大变得更严重

![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250515213437996.png)

- 其中 $Q_{t}(s,a)-V_{\star}(s)$ 设为在 $[-1,1]$ 区间均匀分布
- $Q^\prime$ 函数是另一组独立训练的价值函数

### Q-learning 中过估计的例子
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250515213820173.png)

> 第二列可以看到取 max 之后很多价值非常高，会导致过高估计，让网络拟合到这些数值

- 使用不同的网络来估值和决策
$$
\begin{align}
DQN:\ y_{t}&=r_{t}+\gamma Q_{\theta}(s_{t+1},\arg \max_{a^\prime}Q_{\theta}(s_{t+1},a^\prime)) \\
 \\
Double\ DQN:\ y_{t}&=r_{t}+\gamma Q_{\theta^\prime}(s_{t+1},\arg\max_{a^\prime}Q_{\theta}(s_{t+1},a^\prime))
\end{align}
$$
> 这样 DQN 是运动员，Double DQN 是裁判

### 在 Atari 环境中的实验结果
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250515214956407.png)

> 可以发现 Double DQN 的估计值更接近真实价值，但是 DDQN 的损失会更大一点，因为两个网络参数量更多
> 
> 本人理解，单一模型存在估计过高问题，双模型能够缓解，是因为两个模型之间有一定对抗性，对于过高的动作，在另一个模型评估的时候可能没有这样高，也就是说很难两个模型同时对某一动作都有高的离谱的估计。这样两个模型相互影响来学习

## Dueling DQN
假设动作值函数服从某个分布：
$$
Q(s,a)\sim N(\mu,\sigma)
$$
显然：$V(s)=\mathbb{E}[Q(s,a)]=\mu$
同样有：$Q(s,a)=\mu+\varepsilon(s,a)$，$\varepsilon(s,a)$ 是偏移量

**问题**
如何描述 $\varepsilon$  $\to$ $\varepsilon(s,a)=Q(s,a)-V(s)\to$ 也称为Advantage 函数

Q 函数的问题是难以训练，我们可以使用 $\varepsilon$ 和 V 来表示 Q

![](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250515220755641.png)


### 网络结构
![image.png](https://cdn.jsdelivr.net/gh/AustinSuun/image/img/20250515221007557.png)

> 网络的输出分成两个分支，一条输出 V 函数值，一条输出 A 函数值，再把两条和到一起变成 Q 函数值
> 这样的好处：对于 Q 本身来说，对任何一个动作 a，要做出一个好的估计, 他需要依赖于（s, a）本身的表征的学习。但是如果 V+A 的话，对于一些 a 见得很少，只需要让 A 向向 0 走就可以，让 Q 回归到 V 的值，是对于Q 来说，他不一定回归到 0（这个状态可能没有足够训练，Q 的数值是不准的，可能过高|过低）
> 
> 这就是在网络结构上能够提升的地方 

# 确定性策略梯度